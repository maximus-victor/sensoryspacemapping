# TODO
# webers law -> linear -> story: probably more complex -> we are gonna use this dataset to investigate this.
# we have this data -> we have this ide -> does it work at all -> THIS IS A PILOT
# how does it work? How does it relate?
# Facet Grid -> plot all models
# -> do it for 1, 2, 3, 4 paras
# LLH for the whole dataset -> sum of all
# 64 likelihoods of those
# more parameters -> llh should always go up
# BIC -> for model comparison
# AIC as well?
--> function that takes model, #params, #subj id
# -> cross validation of likelihoods (very last thing)
# which parameter amout is best for every subject -> lets look at the average
# pipe the results to a file


# 80 - 20 CV
# take parameters on the 20 left out trials
# look at predictions of
# autocorrelation
# only model for one subject
# every fith trial out

# stop with the other models - only look at efficient coding v1 and v2
# brain is representing the two stimuli seperately - what happens if we compare them

# maybe look at derivatives for the interesting cases

# plot all the sigma polynomials in one plot -> for interesting cases


# plot a histogram for the lowest values for every subject - how many subjects score best with which parameter amount.

# over vs underfitting
# -> cv, AIC, BIC
# -> let us not fit noise/assume we do not fit noise
# --> out of sample prediction

# --> do not look at LLH within the dataset

# histogram over the max's

TODO: UPDATE
- opt_dyn_noise_sigma
- opt_dyn_mem_noise_sigma
- dyn_noise_pred_choice
- dyn_noise_mem_pred_choice

- model_comp_TEST_LL --> LOOK NEXT
- dyn_noise_histogram_TEST_LL
- dyn_noise_mem_histogram_TEST_LL

Poly Probit initial parameters
[[15.],
[15., .5],
[1e-3, 1., 2],
[1e-3, 1., 2, 1e-3],
[1e-3, 1., 2, 1e-3, 1e-5],
[1e-3, 1., 2, 1e-3, 1e-5, 1e-8],
[1e-3, 1., 2, 1e-3, 1e-5, 1e-8, 1e-8],
[1e-3, 1., 2, 1e-3, 1e-3, 1e-8, 1e-8, 1e-8]]

Efficient coding initial parameters
[[3.0],
[1e-3, 1.],
[1e-3, 1., 2],
[1e-3, 1., 2, 1e-3],
[1e-3, 1., 2, 1e-3, 1e-5],
[1e-3, 1., 2, 1e-3, 1e-5, 1e-8],
[1e-3, 1., 2, 1e-3, 1e-5, 1e-8, 1e-8],
[1e-3, 1., 2, 1e-3, 1e-3, 1e-8, 1e-8, 1e-8]]

Efficient individual coding initial parameters
[[3.0, 3.0],
[1e-3, 1., 1e-3, 1.],
[1e-3, 1., 2, 1e-3, 1., 2],
[1e-3, 1., 2, 1e-3, 1e-3, 1., 2, 1e-3],
[1e-3, 1., 2, 1e-3, 1e-5, 1e-3, 1., 2, 1e-3, 1e-5],
[1e-3, 1., 2, 1e-3, 1e-5, 1e-8, 1e-3, 1., 2, 1e-3, 1e-5, 1e-8],
[1e-3, 1., 2, 1e-3, 1e-5, 1e-8, 1e-8, 1e-3, 1., 2, 1e-3, 1e-5, 1e-8, 1e-8],
[1e-3, 1., 2, 1e-3, 1e-3, 1e-8, 1e-8, 1e-8, 1e-3, 1., 2, 1e-3, 1e-3, 1e-8, 1e-8, 1e-8]]


fit_mean_function inits:
[[1e-3, 8.],
[1e-3, 1., 8.],
[3., 1., 1e-3, 8.],
[3., 1., 1e-3, 1e-5, 8.],
[3., 1., 1e-3, 1e-5, 1e-5, 8.],
[3., 1., 1e-3, 1e-5, 1e-6, 1e-6, 8.],
[3., 1., 1e-3, 1e-5, 1e-6, 1e-6, 1e-7, 8.]]

fit_mean_variance_pred inits:
[[3., 3.],
[3., 3., 3.],
[3., 3., 3., 3.],
[3., 3., 3., 3., 3.],
[3., 3., 3., 3., 3., 3.],
[3., 3., 3., 3., 3., 3., 3.],
[3., 3., 3., 3., 3., 3., 3., 3.]]
